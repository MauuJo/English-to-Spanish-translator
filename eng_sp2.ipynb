{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "step 0: train loss 11.1831, val loss 11.1806\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "step 10: train loss 2.0920, val loss 2.0215\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "step 20: train loss 1.0396, val loss 1.0560\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "step 30: train loss 0.9688, val loss 1.0079\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "step 40: train loss 0.9244, val loss 0.9283\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "step 50: train loss 0.8583, val loss 0.8307\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "step 60: train loss 0.8244, val loss 0.8127\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "step 70: train loss 0.7993, val loss 0.8009\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "step 80: train loss 0.7760, val loss 0.7638\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "step 90: train loss 0.7183, val loss 0.7407\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('english_to_spanish.csv')\n",
    "english_sentences = df['english'].tolist()\n",
    "spanish_sentences = df['spanish'].tolist()\n",
    "\n",
    "# Tokenize sentences using TensorFlow\n",
    "tokenizer_en = tf.keras.preprocessing.text.Tokenizer(filters='', lower=True)\n",
    "tokenizer_es = tf.keras.preprocessing.text.Tokenizer(filters='', lower=True)\n",
    "\n",
    "tokenizer_en.fit_on_texts(english_sentences)\n",
    "tokenizer_es.fit_on_texts(spanish_sentences)\n",
    "\n",
    "# Add special tokens to the tokenizer's vocabulary\n",
    "special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "for token in special_tokens:\n",
    "    tokenizer_en.word_index[token] = len(tokenizer_en.word_index) + 1\n",
    "    tokenizer_es.word_index[token] = len(tokenizer_es.word_index) + 1\n",
    "\n",
    "vocab_size_en = len(tokenizer_en.word_index) + 1\n",
    "vocab_size_es = len(tokenizer_es.word_index) + 1\n",
    "\n",
    "# Convert sentences to sequences of integers\n",
    "english_sequences = tokenizer_en.texts_to_sequences(english_sentences)\n",
    "spanish_sequences = tokenizer_es.texts_to_sequences(spanish_sentences)\n",
    "\n",
    "# Add <sos> and <eos> tokens\n",
    "english_sequences = [[tokenizer_en.word_index['<sos>']] + seq + [tokenizer_en.word_index['<eos>']] for seq in english_sequences]\n",
    "spanish_sequences = [[tokenizer_es.word_index['<sos>']] + seq + [tokenizer_es.word_index['<eos>']] for seq in spanish_sequences]\n",
    "\n",
    "# Pad sequences\n",
    "max_length_en = max(len(seq) for seq in english_sequences)\n",
    "max_length_es = max(len(seq) for seq in spanish_sequences)\n",
    "\n",
    "english_padded = tf.keras.preprocessing.sequence.pad_sequences(english_sequences, maxlen=max_length_en + 2, padding='post', value=tokenizer_en.word_index['<pad>'])\n",
    "spanish_padded = tf.keras.preprocessing.sequence.pad_sequences(spanish_sequences, maxlen=max_length_es + 2, padding='post', value=tokenizer_es.word_index['<pad>'])\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data_es, val_data_es, train_data_en, val_data_en = train_test_split(spanish_padded, english_padded, test_size=0.1, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_data_es = torch.tensor(train_data_es, dtype=torch.long)\n",
    "val_data_es = torch.tensor(val_data_es, dtype=torch.long)\n",
    "train_data_en = torch.tensor(train_data_en, dtype=torch.long)\n",
    "val_data_en = torch.tensor(val_data_en, dtype=torch.long)\n",
    "\n",
    "# Encoding and decoding functions for integer sequences to sentences and vice versa\n",
    "def encode_sentence(sentence, tokenizer, add_special_tokens=True):\n",
    "    tokens = tokenizer.texts_to_sequences([sentence.lower()])[0]\n",
    "    if add_special_tokens:\n",
    "        tokens = [tokenizer.word_index['<sos>']] + tokens + [tokenizer.word_index['<eos>']]\n",
    "    return tokens\n",
    "\n",
    "def decode_sequence(sequence, tokenizer):\n",
    "    reverse_word_index = {index: word for word, index in tokenizer.word_index.items()}\n",
    "    decoded_sentence = ' '.join([reverse_word_index.get(i, '<unk>') for i in sequence if i not in [tokenizer.word_index['<pad>'], tokenizer.word_index['<sos>'], tokenizer.word_index['<eos>']]])\n",
    "    return decoded_sentence\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "block_size = max(max_length_en, max_length_es) + 2  # account for <sos> and <eos>\n",
    "max_iters = 100\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Data loading\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data_en, data_es = train_data_en, train_data_es\n",
    "    else:\n",
    "        data_en, data_es = val_data_en, val_data_es\n",
    "\n",
    "    ix = torch.randint(len(data_en), (batch_size,))\n",
    "    x = torch.stack([data_en[i] for i in ix])\n",
    "    y = torch.stack([data_es[i] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size, mask_future=True):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.mask_future = mask_future\n",
    "\n",
    "    def forward(self, x, context=None):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x if context is None else context)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x if context is None else context)\n",
    "\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        if self.mask_future:\n",
    "            wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, mask_future=True):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, mask_future) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class CrossAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of cross-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size, mask_future=False) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context):\n",
    "        out = torch.cat([h(x, context) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attn = SelfAttention(n_head, head_size, mask_future=False)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.self_attn(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer decoder block\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attn = SelfAttention(n_head, head_size, mask_future=True)\n",
    "        self.cross_attn = CrossAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        self.ln3 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x, encoder_output):\n",
    "        x = x + self.self_attn(self.ln1(x))\n",
    "        x = x + self.cross_attn(self.ln2(x), encoder_output)\n",
    "        x = x + self.ffwd(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table_en = nn.Embedding(vocab_size_en, n_embd)\n",
    "        self.token_embedding_table_es = nn.Embedding(vocab_size_es, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.encoder_blocks = nn.Sequential(*[EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.decoder_blocks = nn.Sequential(*[DecoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size_es)\n",
    "\n",
    "    def forward(self, src, tgt=None):\n",
    "        B, T_src = src.shape\n",
    "        tok_emb_src = self.token_embedding_table_en(src)\n",
    "        pos_emb_src = self.position_embedding_table(torch.arange(T_src, device=src.device))\n",
    "        x = tok_emb_src + pos_emb_src\n",
    "        encoder_output = self.encoder_blocks(x)\n",
    "\n",
    "        if tgt is not None:\n",
    "            T_tgt = tgt.shape[1]\n",
    "            tok_emb_tgt = self.token_embedding_table_es(tgt)\n",
    "            pos_emb_tgt = self.position_embedding_table(torch.arange(T_tgt, device=tgt.device))\n",
    "            decoder_input = tok_emb_tgt + pos_emb_tgt\n",
    "            decoder_output = decoder_input\n",
    "            for decoder_block in self.decoder_blocks:\n",
    "                decoder_output = decoder_block(decoder_output, encoder_output)\n",
    "            x = decoder_output\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if tgt is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            tgt = tgt.view(B * T)\n",
    "            loss = F.cross_entropy(logits, tgt, ignore_index=0)  # ignore padding index\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, src, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            src_cond = src[:, -block_size:]\n",
    "            logits, _ = self(src_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            src_next = torch.multinomial(probs, num_samples=1)\n",
    "            src = torch.cat((src, src_next), dim=1)\n",
    "        return src\n",
    "\n",
    "model = TransformerLanguageModel()\n",
    "\n",
    "# Create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    print(iter)\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # Sample batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# context = torch.zeros((1, 1), dtype=torch.long)\n",
    "# print(decode_sequence(model.generate(context, max_new_tokens=500)[0].tolist(), tokenizer_es))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Spanish sentence: parece silbes podemos\n"
     ]
    }
   ],
   "source": [
    "def test_model(input_sentence, model, tokenizer_en, tokenizer_es):\n",
    "   \n",
    "    input_sequence = encode_sentence(input_sentence, tokenizer_en)\n",
    "    input_tensor = torch.tensor([input_sequence], dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_logits, _ = model(input_tensor)\n",
    "    \n",
    "    predicted_sequence = torch.argmax(output_logits, dim=-1).squeeze().tolist()\n",
    "    predicted_sentence = decode_sequence(predicted_sequence, tokenizer_es)\n",
    "    \n",
    "    return predicted_sentence\n",
    "\n",
    "\n",
    "input_sentence = \"Hello\"\n",
    "\n",
    "\n",
    "predicted_spanish_sentence = test_model(input_sentence, model, tokenizer_en, tokenizer_es)\n",
    "print(\"Predicted Spanish sentence:\", predicted_spanish_sentence)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
